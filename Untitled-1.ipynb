{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82984f1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#!/usr/bin/env python3\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mPure ETL Pipeline Web Frontend\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[33;03mA data engineering focused ETL that extracts and structures data without \u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03mmaking analytical assumptions or classifications.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mst\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Pure ETL Pipeline Web Frontend\n",
    "\n",
    "A data engineering focused ETL that extracts and structures data without \n",
    "making analytical assumptions or classifications.\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "# Only use standard libraries and basic packages\n",
    "try:\n",
    "    import PyPDF2\n",
    "    PDF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PDF_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import openpyxl\n",
    "    EXCEL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    EXCEL_AVAILABLE = False\n",
    "\n",
    "class PureETL:\n",
    "    \"\"\"Pure data engineering ETL - extraction and structuring only.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the pure ETL.\"\"\"\n",
    "        self.setup_directories()\n",
    "        self.load_history()\n",
    "    \n",
    "    def setup_directories(self):\n",
    "        \"\"\"Setup required directories.\"\"\"\n",
    "        self.upload_dir = Path(\"pure_etl_uploads\")\n",
    "        self.output_dir = Path(\"pure_etl_outputs\")\n",
    "        self.history_file = Path(\"pure_etl_history.json\")\n",
    "        \n",
    "        self.upload_dir.mkdir(exist_ok=True)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def load_history(self):\n",
    "        \"\"\"Load processing history.\"\"\"\n",
    "        if self.history_file.exists():\n",
    "            try:\n",
    "                with open(self.history_file, 'r') as f:\n",
    "                    self.history = json.load(f)\n",
    "            except:\n",
    "                self.history = []\n",
    "        else:\n",
    "            self.history = []\n",
    "    \n",
    "    def save_history(self):\n",
    "        \"\"\"Save processing history.\"\"\"\n",
    "        with open(self.history_file, 'w') as f:\n",
    "            json.dump(self.history, f, indent=2, default=str)\n",
    "    \n",
    "    def extract_pdf_text(self, file_path):\n",
    "        \"\"\"Extract text from PDF.\"\"\"\n",
    "        if not PDF_AVAILABLE:\n",
    "            return \"PDF processing not available. Install PyPDF2.\"\n",
    "        \n",
    "        try:\n",
    "            text = \"\"\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            return f\"Error reading PDF: {str(e)}\"\n",
    "    \n",
    "    def extract_excel_text(self, file_path):\n",
    "        \"\"\"Extract text from Excel.\"\"\"\n",
    "        if not EXCEL_AVAILABLE:\n",
    "            return \"Excel processing not available. Install openpyxl.\"\n",
    "        \n",
    "        try:\n",
    "            text = \"\"\n",
    "            df_dict = pd.read_excel(file_path, sheet_name=None)\n",
    "            for sheet_name, df in df_dict.items():\n",
    "                text += f\"Sheet: {sheet_name}\\n\"\n",
    "                for _, row in df.iterrows():\n",
    "                    row_text = \" \".join([str(val) for val in row.values if pd.notna(val)])\n",
    "                    if row_text.strip():\n",
    "                        text += row_text + \"\\n\"\n",
    "                text += \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            return f\"Error reading Excel: {str(e)}\"\n",
    "    \n",
    "    def extract_text_file(self, file_path):\n",
    "        \"\"\"Extract text from text file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                    return f.read()\n",
    "            except Exception as e:\n",
    "                return f\"Error reading text file: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error reading text file: {str(e)}\"\n",
    "    \n",
    "    def segment_sentences(self, text):\n",
    "        \"\"\"Simple sentence segmentation.\"\"\"\n",
    "        # Split on sentence endings\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        \n",
    "        # Clean sentences\n",
    "        clean_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            sentence = re.sub(r'\\s+', ' ', sentence)  # Normalize whitespace\n",
    "            if len(sentence) > 10:  # Minimum length\n",
    "                clean_sentences.append(sentence)\n",
    "        \n",
    "        return clean_sentences\n",
    "    \n",
    "    def extract_speaker_raw(self, text):\n",
    "        \"\"\"Extract speaker patterns without classification.\"\"\"\n",
    "        # Look for speaker patterns at start of text\n",
    "        patterns = [\n",
    "            r'^([A-Z][A-Z\\s]+):\\s*',  # ALL CAPS:\n",
    "            r'^([A-Z][a-z]+\\s+[A-Z][a-z]+):\\s*',  # First Last:\n",
    "            r'^(CEO|CFO|Chief Executive|Chief Financial|Chief Risk Officer):\\s*',  # Titles\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                return match.group(1).strip()\n",
    "        \n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "    def classify_document_type(self, filename, text):\n",
    "        \"\"\"Enhanced document type classification based on filename and content patterns.\"\"\"\n",
    "        filename_lower = filename.lower()\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # More specific filename patterns\n",
    "        if any(term in filename_lower for term in ['presentation', 'slides', 'deck']):\n",
    "            return 'earnings_presentation'\n",
    "        elif any(term in filename_lower for term in ['supplement', 'financial_supplement', 'fin_supp']):\n",
    "            return 'financial_supplement'\n",
    "        elif any(term in filename_lower for term in ['transcript', 'earnings_call', 'call_transcript']):\n",
    "            return 'earnings_call'\n",
    "        elif any(term in filename_lower for term in ['report', 'quarterly_report', 'annual_report']):\n",
    "            return 'financial_report'\n",
    "        elif any(term in filename_lower for term in ['press_release', 'release', 'announcement']):\n",
    "            return 'press_release'\n",
    "        elif filename_lower.endswith(('.xlsx', '.xls')):\n",
    "            # Excel files are likely financial supplements or data\n",
    "            if any(term in text_lower for term in ['balance sheet', 'income statement', 'cash flow']):\n",
    "                return 'financial_supplement'\n",
    "            else:\n",
    "                return 'financial_data'\n",
    "        elif filename_lower.endswith('.pdf'):\n",
    "            # PDF content analysis - more specific patterns\n",
    "            if ('transcript' in text_lower and any(term in text_lower for term in ['operator:', 'moderator:', 'q&a'])):\n",
    "                return 'earnings_call'\n",
    "            elif any(term in text_lower for term in ['slide', 'presentation', 'agenda']):\n",
    "                return 'earnings_presentation'\n",
    "            elif any(term in text_lower for term in ['balance sheet', 'income statement', 'financial highlights']):\n",
    "                return 'financial_supplement'\n",
    "            elif any(term in text_lower for term in ['press release', 'announces', 'reported earnings']):\n",
    "                return 'press_release'\n",
    "            else:\n",
    "                return 'financial_document'\n",
    "        else:\n",
    "            # Text files and others\n",
    "            if any(term in text_lower for term in ['transcript', 'operator:', 'moderator:', 'q&a session']):\n",
    "                return 'earnings_call'\n",
    "            else:\n",
    "                return 'text_document'\n",
    "    \n",
    "    def extract_raw_features(self, df):\n",
    "        \"\"\"Extract raw features without analytical assumptions.\"\"\"\n",
    "        \n",
    "        # Handle missing values first\n",
    "        df['text'] = df['text'].fillna('').astype(str)\n",
    "        df['speaker_raw'] = df['speaker_raw'].fillna('UNKNOWN').astype(str)\n",
    "        \n",
    "        # Basic text metrics\n",
    "        df['word_count'] = df['text'].str.split().str.len().fillna(0).astype(int)\n",
    "        df['char_count'] = df['text'].str.len().fillna(0).astype(int)\n",
    "        df['sentence_length'] = df['word_count']  # Alias for clarity\n",
    "        \n",
    "        # Raw financial terms extraction (no classification)\n",
    "        financial_vocabulary = [\n",
    "            'revenue', 'income', 'profit', 'earnings', 'billion', 'million', \n",
    "            'eps', 'capital', 'assets', 'growth', 'performance', 'margin',\n",
    "            'return', 'yield', 'dividend', 'interest', 'loan', 'credit',\n",
    "            'deposit', 'fee', 'commission', 'expense', 'cost', 'investment',\n",
    "            'portfolio', 'risk', 'regulatory', 'compliance', 'basel',\n",
    "            'tier', 'ratio', 'liquidity', 'solvency', 'provision'\n",
    "        ]\n",
    "        \n",
    "        def extract_terms_found(text):\n",
    "            \"\"\"Extract terms found without interpretation.\"\"\"\n",
    "            if pd.isna(text) or text == '':\n",
    "                return 'NONE'\n",
    "            \n",
    "            text_lower = str(text).lower()\n",
    "            found_terms = []\n",
    "            for term in financial_vocabulary:\n",
    "                if term in text_lower:\n",
    "                    found_terms.append(term)\n",
    "            return '|'.join(found_terms) if found_terms else 'NONE'\n",
    "        \n",
    "        df['all_financial_terms'] = df['text'].apply(extract_terms_found)\n",
    "        \n",
    "        # Raw financial figures extraction (no interpretation)\n",
    "        def extract_figures_found(text):\n",
    "            \"\"\"Extract numerical figures without interpretation.\"\"\"\n",
    "            if pd.isna(text) or text == '':\n",
    "                return 'NONE'\n",
    "                \n",
    "            import re\n",
    "            text_str = str(text)\n",
    "            \n",
    "            # Patterns for numerical figures\n",
    "            patterns = [\n",
    "                r'\\$[\\d,]+\\.?\\d*\\s*(?:billion|million|thousand|B|M|K)?',  # Dollar amounts\n",
    "                r'[\\d,]+\\.?\\d*\\s*(?:billion|million|thousand|percent|%|basis points|bps)',  # Numbers with units\n",
    "                r'[\\d,]+\\.?\\d*\\s*(?:dollars|cents)',  # Dollar/cent amounts\n",
    "                r'(?:approximately|about|around|roughly)\\s*[\\d,]+\\.?\\d*',  # Approximate figures\n",
    "            ]\n",
    "            \n",
    "            figures = []\n",
    "            for pattern in patterns:\n",
    "                matches = re.findall(pattern, text_str, re.IGNORECASE)\n",
    "                figures.extend(matches)\n",
    "            \n",
    "            return '|'.join(figures) if figures else 'NONE'\n",
    "        \n",
    "        df['financial_figures'] = df['text'].apply(extract_figures_found)\n",
    "        df['financial_figures_text'] = df['financial_figures']  # Compatibility\n",
    "        \n",
    "        # Raw temporal indicators (no classification)\n",
    "        def extract_temporal_indicators(text):\n",
    "            \"\"\"Extract temporal language without classifying as actual/projection.\"\"\"\n",
    "            if pd.isna(text) or text == '':\n",
    "                return 'NONE'\n",
    "                \n",
    "            text_lower = str(text).lower()\n",
    "            \n",
    "            temporal_terms = [\n",
    "                'expect', 'forecast', 'project', 'anticipate', 'estimate',\n",
    "                'guidance', 'outlook', 'target', 'goal', 'plan', 'intend',\n",
    "                'will be', 'should be', 'likely to', 'going forward',\n",
    "                'next quarter', 'next year', 'future', 'upcoming',\n",
    "                'reported', 'achieved', 'delivered', 'recorded', 'posted',\n",
    "                'was', 'were', 'had', 'generated', 'earned', 'realized',\n",
    "                'last quarter', 'previous', 'year-over-year', 'compared to'\n",
    "            ]\n",
    "            \n",
    "            found_terms = []\n",
    "            for term in temporal_terms:\n",
    "                if term in text_lower:\n",
    "                    found_terms.append(term)\n",
    "            \n",
    "            return '|'.join(found_terms) if found_terms else 'NONE'\n",
    "        \n",
    "        df['temporal_indicators'] = df['text'].apply(extract_temporal_indicators)\n",
    "        \n",
    "        # Basic boolean flags (factual, not interpretive)\n",
    "        df['has_financial_terms'] = (df['all_financial_terms'] != 'NONE').astype(bool)\n",
    "        df['has_financial_figures'] = (df['financial_figures'] != 'NONE').astype(bool)\n",
    "        df['has_temporal_language'] = (df['temporal_indicators'] != 'NONE').astype(bool)\n",
    "        df['has_speaker_identified'] = (df['speaker_raw'] != 'UNKNOWN').astype(bool)\n",
    "        df['is_empty_text'] = (df['text'] == '').astype(bool)\n",
    "        \n",
    "        # Ensure no missing values\n",
    "        self._ensure_no_missing_values(df)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _ensure_no_missing_values(self, df):\n",
    "        \"\"\"Ensure no missing values for downstream processing.\"\"\"\n",
    "        \n",
    "        # String columns\n",
    "        string_cols = ['all_financial_terms', 'financial_figures', 'financial_figures_text', \n",
    "                      'temporal_indicators', 'speaker_raw', 'text', 'source_file', \n",
    "                      'institution', 'quarter', 'source_type']\n",
    "        \n",
    "        for col in string_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('NONE' if 'financial' in col or 'temporal' in col else 'UNKNOWN').astype(str)\n",
    "        \n",
    "        # Numeric columns\n",
    "        numeric_cols = ['word_count', 'char_count', 'sentence_length', 'sentence_id']\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(0).astype(int)\n",
    "        \n",
    "        # Boolean columns\n",
    "        boolean_cols = ['has_financial_terms', 'has_financial_figures', 'has_temporal_language', \n",
    "                       'has_speaker_identified', 'is_empty_text']\n",
    "        for col in boolean_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(False).astype(bool)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def process_files(self, institution, quarter, uploaded_files, progress_callback=None, uploaded_by=\"Unknown\"):\n",
    "        \"\"\"Process uploaded files with pure ETL approach.\"\"\"\n",
    "        try:\n",
    "            # Create processing directory\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            process_dir = self.upload_dir / f\"{institution}_{quarter}_{timestamp}\"\n",
    "            process_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            all_records = []\n",
    "            file_names = []\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(0.1, \"Saving uploaded files...\")\n",
    "            \n",
    "            # Process each file\n",
    "            for i, uploaded_file in enumerate(uploaded_files):\n",
    "                file_path = process_dir / uploaded_file.name\n",
    "                \n",
    "                # Save file\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(uploaded_file.getbuffer())\n",
    "                \n",
    "                file_names.append(uploaded_file.name)\n",
    "                \n",
    "                if progress_callback:\n",
    "                    progress_callback(0.2 + (i * 0.4 / len(uploaded_files)), \n",
    "                                    f\"Processing {uploaded_file.name}...\")\n",
    "                \n",
    "                # Extract text based on file type\n",
    "                if uploaded_file.name.lower().endswith('.pdf'):\n",
    "                    text = self.extract_pdf_text(file_path)\n",
    "                elif uploaded_file.name.lower().endswith(('.xlsx', '.xls')):\n",
    "                    text = self.extract_excel_text(file_path)\n",
    "                elif uploaded_file.name.lower().endswith('.txt'):\n",
    "                    text = self.extract_text_file(file_path)\n",
    "                else:\n",
    "                    text = f\"Unsupported file type: {uploaded_file.name}\"\n",
    "                \n",
    "                if not text or text.startswith(\"Error\") or len(text.strip()) < 10:\n",
    "                    st.warning(f\"Could not extract meaningful text from {uploaded_file.name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Segment into sentences\n",
    "                sentences = self.segment_sentences(text)\n",
    "                \n",
    "                # Create records\n",
    "                doc_type = self.classify_document_type(uploaded_file.name, text)\n",
    "                \n",
    "                for idx, sentence in enumerate(sentences):\n",
    "                    speaker = self.extract_speaker_raw(sentence)\n",
    "                    \n",
    "                    record = {\n",
    "                        'source_file': uploaded_file.name,\n",
    "                        'institution': institution,\n",
    "                        'quarter': quarter,\n",
    "                        'sentence_id': idx + 1,\n",
    "                        'speaker_raw': speaker,\n",
    "                        'text': sentence,\n",
    "                        'source_type': doc_type,\n",
    "                        'call_id': f\"{institution}_{quarter}_{timestamp}\",\n",
    "                        'file_path': str(file_path),\n",
    "                        'processing_date': datetime.now().isoformat(),\n",
    "                        'extraction_timestamp': datetime.now().isoformat()\n",
    "                    }\n",
    "                    all_records.append(record)\n",
    "            \n",
    "            if not all_records:\n",
    "                raise Exception(\"No text content could be extracted from uploaded files\")\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(0.7, \"Creating structured dataset...\")\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(all_records)\n",
    "            \n",
    "            # Add raw features (no analysis)\n",
    "            if progress_callback:\n",
    "                progress_callback(0.8, \"Extracting raw features...\")\n",
    "            \n",
    "            df = self.extract_raw_features(df)\n",
    "            \n",
    "            # Save outputs\n",
    "            output_dir = self.output_dir / f\"{institution}_{quarter}_{timestamp}\"\n",
    "            output_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            output_files = []\n",
    "            \n",
    "            # Pure ETL dataset with taxonomy naming\n",
    "            quarter_clean = quarter.replace(\" \", \"_\")\n",
    "            etl_file = output_dir / f\"{institution}_{quarter_clean}_PureETL_{uploaded_by}_{timestamp}.csv\"\n",
    "            df.to_csv(etl_file, index=False)\n",
    "            output_files.append(str(etl_file))\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(1.0, \"ETL processing complete!\")\n",
    "            \n",
    "            # Add to history\n",
    "            self.history.insert(0, {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'institution': institution,\n",
    "                'quarter': quarter,\n",
    "                'uploaded_by': uploaded_by,\n",
    "                'files': file_names,\n",
    "                'output_files': output_files,\n",
    "                'status': 'Success',\n",
    "                'record_count': len(df),\n",
    "                'approach': 'Pure ETL - Standardized taxonomy'\n",
    "            })\n",
    "            self.save_history()\n",
    "            \n",
    "            return output_files, len(df), \"Success\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Add failed record to history\n",
    "            self.history.insert(0, {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'institution': institution,\n",
    "                'quarter': quarter,\n",
    "                'uploaded_by': uploaded_by,\n",
    "                'files': file_names if 'file_names' in locals() else [],\n",
    "                'output_files': [],\n",
    "                'status': 'Failed',\n",
    "                'record_count': 0,\n",
    "                'error': error_msg,\n",
    "                'approach': 'Pure ETL - Standardized taxonomy'\n",
    "            })\n",
    "            self.save_history()\n",
    "            \n",
    "            return [], 0, error_msg\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main Streamlit application.\"\"\"\n",
    "    \n",
    "    st.set_page_config(\n",
    "        page_title=\"Pure ETL Pipeline\",\n",
    "        page_icon=\"ðŸ”§\",\n",
    "        layout=\"wide\"\n",
    "    )\n",
    "    \n",
    "    # Initialize ETL\n",
    "    etl = PureETL()\n",
    "    \n",
    "    # Header\n",
    "    st.title(\"ðŸ”§ Pure Financial Document ETL Pipeline\")\n",
    "    st.markdown(\"**Data Engineering Focus** - Extract and structure data without analytical assumptions\")\n",
    "    \n",
    "    # Taxonomy guide link\n",
    "    st.info(\"ðŸ“‹ **New!** Follow our [Taxonomy Guide](TAXONOMY_GUIDE.md) for standardized file naming and organization\")\n",
    "    \n",
    "    # Check dependencies\n",
    "    missing_deps = []\n",
    "    if not PDF_AVAILABLE:\n",
    "        missing_deps.append(\"PyPDF2 (for PDF processing)\")\n",
    "    if not EXCEL_AVAILABLE:\n",
    "        missing_deps.append(\"openpyxl (for Excel processing)\")\n",
    "    \n",
    "    if missing_deps:\n",
    "        st.warning(f\"Optional dependencies missing: {', '.join(missing_deps)}\")\n",
    "        st.info(\"Install with: `pip install PyPDF2 openpyxl`\")\n",
    "    \n",
    "    # Sidebar\n",
    "    with st.sidebar:\n",
    "        st.header(\"ðŸ›ï¸ Institution Processing\")\n",
    "        \n",
    "        # Institution selection with standardized names\n",
    "        institutions = [\n",
    "            \"JPMorgan\", \"BankOfAmerica\", \"Citigroup\", \"WellsFargo\", \"GoldmanSachs\",\n",
    "            \"MorganStanley\", \"USBancorp\", \"TrustFinancial\", \"PNCFinancial\", \"CapitalOne\",\n",
    "            \"HSBC\", \"Barclays\", \"Lloyds\", \"RoyalBankScotland\", \"StandardChartered\",\n",
    "            \"Deutsche\", \"UBS\", \"CreditSuisse\", \"BNPParibas\", \"SocGen\",\n",
    "            \"Other (Custom)\"\n",
    "        ]\n",
    "        \n",
    "        institution_choice = st.selectbox(\n",
    "            \"Institution Name\",\n",
    "            institutions,\n",
    "            help=\"Select standardized institution name\"\n",
    "        )\n",
    "        \n",
    "        # Custom institution input if \"Other\" selected\n",
    "        if institution_choice == \"Other (Custom)\":\n",
    "            institution = st.text_input(\n",
    "                \"Custom Institution\",\n",
    "                placeholder=\"e.g., RegionalBank\",\n",
    "                help=\"Use CamelCase format (no spaces)\"\n",
    "            )\n",
    "        else:\n",
    "            institution = institution_choice\n",
    "        \n",
    "        # Quarter and Year selection\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            quarter = st.selectbox(\"Quarter\", [\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "        with col2:\n",
    "            year = st.selectbox(\"Year\", [\"2025\", \"2024\", \"2026\", \"2023\"])\n",
    "        \n",
    "        # User identification\n",
    "        uploaded_by = st.text_input(\n",
    "            \"Uploaded By\",\n",
    "            placeholder=\"e.g., JohnSmith\",\n",
    "            help=\"Your name/ID (CamelCase format)\"\n",
    "        )\n",
    "        \n",
    "        # Combine quarter and year for processing\n",
    "        quarter_year = f\"{quarter} {year}\"\n",
    "        \n",
    "        # File upload\n",
    "        st.subheader(\"ðŸ“ Document Upload\")\n",
    "        uploaded_files = st.file_uploader(\n",
    "            \"Upload Documents\",\n",
    "            type=['pdf', 'xlsx', 'xls', 'txt'],\n",
    "            accept_multiple_files=True,\n",
    "            help=\"Upload financial documents\"\n",
    "        )\n",
    "        \n",
    "        # Process button\n",
    "        process_button = st.button(\n",
    "            \"ðŸ”§ Extract & Structure Data\",\n",
    "            disabled=not (institution and uploaded_files and uploaded_by),\n",
    "            type=\"primary\"\n",
    "        )\n",
    "    \n",
    "    # Main content\n",
    "    col1, col2 = st.columns([2, 1])\n",
    "    \n",
    "    with col1:\n",
    "        st.header(\"ðŸ“‹ ETL Processing Status\")\n",
    "        \n",
    "        if process_button:\n",
    "            if not institution:\n",
    "                st.error(\"Please enter an institution name\")\n",
    "            elif not uploaded_files:\n",
    "                st.error(\"Please upload at least one document\")\n",
    "            elif not uploaded_by:\n",
    "                st.error(\"Please enter your name/ID in 'Uploaded By' field\")\n",
    "            else:\n",
    "                # Processing\n",
    "                st.info(f\"Processing {len(uploaded_files)} documents for {institution} {quarter_year} by {uploaded_by}...\")\n",
    "                \n",
    "                # Progress tracking\n",
    "                progress_bar = st.progress(0)\n",
    "                status_text = st.empty()\n",
    "                \n",
    "                def update_progress(value, message):\n",
    "                    progress_bar.progress(value)\n",
    "                    status_text.text(message)\n",
    "                \n",
    "                # Process files\n",
    "                output_files, record_count, status = etl.process_files(\n",
    "                    institution, quarter_year, uploaded_files, update_progress, uploaded_by\n",
    "                )\n",
    "                \n",
    "                if status == \"Success\":\n",
    "                    st.success(f\"âœ… ETL processing completed successfully!\")\n",
    "                    st.info(f\"Extracted {record_count:,} structured records\")\n",
    "                    \n",
    "                    # Download section\n",
    "                    st.subheader(\"ðŸ“¥ Download Pure ETL Dataset\")\n",
    "                    for output_file in output_files:\n",
    "                        file_path = Path(output_file)\n",
    "                        if file_path.exists():\n",
    "                            with open(file_path, 'rb') as f:\n",
    "                                st.download_button(\n",
    "                                    label=f\"ðŸ“„ Download {file_path.name}\",\n",
    "                                    data=f.read(),\n",
    "                                    file_name=file_path.name,\n",
    "                                    mime=\"text/csv\"\n",
    "                                )\n",
    "                else:\n",
    "                    st.error(f\"âŒ ETL processing failed: {status}\")\n",
    "        \n",
    "        else:\n",
    "            st.info(\"ðŸ‘† Upload documents and click 'Extract & Structure Data' to begin\")\n",
    "            \n",
    "            if uploaded_files:\n",
    "                st.subheader(\"ðŸ“ Uploaded Files\")\n",
    "                for file in uploaded_files:\n",
    "                    size_mb = file.size / (1024 * 1024)\n",
    "                    st.write(f\"â€¢ {file.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    with col2:\n",
    "        st.header(\"ðŸ“š Processing History\")\n",
    "        \n",
    "        if etl.history:\n",
    "            for record in etl.history[:8]:  # Show last 8\n",
    "                status_icon = \"âœ…\" if record['status'] == 'Success' else \"âŒ\"\n",
    "                \n",
    "                with st.expander(f\"{status_icon} {record['institution']} - {record['quarter']} - {record.get('uploaded_by', 'Unknown')}\"):\n",
    "                    st.write(f\"**Date:** {record['timestamp'][:19]}\")\n",
    "                    st.write(f\"**Status:** {record['status']}\")\n",
    "                    st.write(f\"**Uploaded By:** {record.get('uploaded_by', 'Unknown')}\")\n",
    "                    st.write(f\"**Files:** {len(record['files'])}\")\n",
    "                    st.write(f\"**Approach:** {record.get('approach', 'Standard')}\")\n",
    "                    \n",
    "                    if record['status'] == 'Success':\n",
    "                        st.write(f\"**Records:** {record['record_count']:,}\")\n",
    "                    else:\n",
    "                        if 'error' in record:\n",
    "                            st.write(f\"**Error:** {record['error'][:100]}...\")\n",
    "                    \n",
    "                    if record['files']:\n",
    "                        st.write(\"**Files:**\")\n",
    "                        for file in record['files']:\n",
    "                            st.write(f\"â€¢ {file}\")\n",
    "        else:\n",
    "            st.info(\"No processing history yet\")\n",
    "    \n",
    "    # Footer\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"**Pure ETL Approach:** Extract â†’ Structure â†’ Export | Analysis happens downstream\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
